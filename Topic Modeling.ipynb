{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29daabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "490d42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('datasets/Lezione_7-Topic_modeling/dataset_Research_Article.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ede0d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = dataset['TITLE'] + dataset['ABSTRACT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadffa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Luigi\\anaconda3\\envs\\Dataset_fakenews\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lavoro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f0ae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(items):\n",
    "    for item in items:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(item), deacc=True))\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in words if word not in stop_words and len(word) >=5] for words in texts]\n",
    "\n",
    "data_words = list(sent_to_words(titles))\n",
    "data_words = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116d2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c7d873",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora \n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c551d75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"graph\" + 0.010*\"graphs\" + 0.007*\"problem\" + 0.007*\"number\" + '\n",
      "  '0.006*\"paper\" + 0.005*\"results\" + 0.005*\"study\" + 0.004*\"space\" + '\n",
      "  '0.004*\"given\" + 0.004*\"theory\"'),\n",
      " (1,\n",
      "  '0.006*\"field\" + 0.006*\"model\" + 0.006*\"phase\" + 0.005*\"magnetic\" + '\n",
      "  '0.005*\"energy\" + 0.005*\"quantum\" + 0.005*\"using\" + 0.004*\"temperature\" + '\n",
      "  '0.004*\"state\" + 0.004*\"density\"'),\n",
      " (2,\n",
      "  '0.008*\"equations\" + 0.008*\"problem\" + 0.007*\"method\" + 0.006*\"algorithm\" + '\n",
      "  '0.006*\"equation\" + 0.005*\"solutions\" + 0.005*\"learning\" + 0.005*\"using\" + '\n",
      "  '0.005*\"paper\" + 0.005*\"results\"'),\n",
      " (3,\n",
      "  '0.013*\"model\" + 0.009*\"algorithm\" + 0.008*\"problem\" + 0.007*\"based\" + '\n",
      "  '0.007*\"method\" + 0.006*\"models\" + 0.006*\"paper\" + 0.006*\"learning\" + '\n",
      "  '0.006*\"approach\" + 0.006*\"using\"'),\n",
      " (4,\n",
      "  '0.013*\"model\" + 0.010*\"based\" + 0.008*\"estimation\" + 0.007*\"models\" + '\n",
      "  '0.007*\"proposed\" + 0.007*\"method\" + 0.007*\"learning\" + 0.006*\"using\" + '\n",
      "  '0.006*\"methods\" + 0.006*\"approach\"'),\n",
      " (5,\n",
      "  '0.011*\"system\" + 0.008*\"systems\" + 0.006*\"design\" + 0.006*\"based\" + '\n",
      "  '0.006*\"software\" + 0.005*\"paper\" + 0.005*\"power\" + 0.005*\"control\" + '\n",
      "  '0.005*\"using\" + 0.004*\"model\"'),\n",
      " (6,\n",
      "  '0.006*\"logic\" + 0.006*\"algorithm\" + 0.005*\"based\" + 0.005*\"paper\" + '\n",
      "  '0.004*\"using\" + 0.004*\"approach\" + 0.004*\"theory\" + 0.004*\"present\" + '\n",
      "  '0.004*\"results\" + 0.003*\"model\"'),\n",
      " (7,\n",
      "  '0.004*\"electron\" + 0.004*\"results\" + 0.003*\"using\" + 0.003*\"cluster\" + '\n",
      "  '0.003*\"clusters\" + 0.003*\"laser\" + 0.003*\"present\" + 0.003*\"system\" + '\n",
      "  '0.003*\"optical\" + 0.003*\"light\"'),\n",
      " (8,\n",
      "  '0.008*\"mathbb\" + 0.007*\"functions\" + 0.006*\"group\" + 0.006*\"function\" + '\n",
      "  '0.006*\"mathcal\" + 0.006*\"finite\" + 0.006*\"paper\" + 0.006*\"prove\" + '\n",
      "  '0.005*\"space\" + 0.005*\"problem\"'),\n",
      " (9,\n",
      "  '0.013*\"learning\" + 0.012*\"network\" + 0.010*\"networks\" + 0.008*\"based\" + '\n",
      "  '0.008*\"neural\" + 0.007*\"model\" + 0.007*\"using\" + 0.005*\"paper\" + '\n",
      "  '0.005*\"information\" + 0.005*\"methods\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from pprint import pprint\n",
    "\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics,\n",
    "                                       passes = 3)\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2a7a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b33d8946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC: 9\n",
      "SCORE:0.98747855\n"
     ]
    }
   ],
   "source": [
    "unseen_document1 = \"NETHIC: A system for automatic text classification using neural networks and hierarchical taxonomies. This paper presents NETHIC, a software system for the automatic classification of textual documents based on hierarchical taxonomies and artificial neural networks. This approach combines the advantages of highly-structured hierarchies of textual labels with the versatility and scalability of neural networks, thus bringing about a textual classifier that displays high levels of performance in terms of both effectiveness and efficiency. The system has first been tested as a general-purpose classifier on a generic document corpus, and then applied to the specific domain tackled by DANTE, a European project that is meant to address criminal and terrorist-related online contents, showing consistent results across both application domains.\"\n",
    "title = id2word.doc2bow(simple_preprocess(unseen_document1))\n",
    "for index, score in lda_model[title]:\n",
    "    print(\"TOPIC: \"+str(index))\n",
    "    print(\"SCORE:\"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0c650c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC: 2\n",
      "SCORE:0.07083743\n",
      "TOPIC: 9\n",
      "SCORE:0.91681325\n"
     ]
    }
   ],
   "source": [
    "unseen_document2 = \"An Automatic Text Classification Method Based on Hierarchical Taxonomies, Neural Networks and Document Embedding: The NETHIC Tool. This work describes an automatic text classification method implemented in a software tool called NETHIC, which takes advantage of the inner capabilities of highly-scalable neural networks combined with the expressiveness of hierarchical taxonomies. As such, NETHIC succeeds in bringing about a mechanism for text classification that proves to be significantly effective as well as efficient. The tool had undergone an experimentation process against both a generic and a domain-specific corpus, outputting promising results. On the basis of this experimentation, NETHIC has been now further refined and extended by adding a document embedding mechanism, which has shown improvements in terms of performance on the individual networks and on the whole hierarchical model.\"\n",
    "title = id2word.doc2bow(simple_preprocess(unseen_document2))\n",
    "for index, score in lda_model[title]:\n",
    "    print(\"TOPIC: \"+str(index))\n",
    "    print(\"SCORE:\"+str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc025d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
